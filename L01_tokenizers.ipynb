{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    background: linear-gradient(90deg,rgb(251, 255, 10), #ff758c, #ff4d6d);\n",
    "    -webkit-background-clip: text;\n",
    "    -webkit-text-fill-color: transparent;\n",
    "    font-size: 20px;\n",
    "    font-weight: bold;\n",
    "    text-align: center;\">\n",
    "  Basic Python\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average age :  27.33\n",
      "Average age :  27.33\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sent = 'They told that their ages are 25 26 and 31 respectively.'\n",
    "\n",
    "#Find the average ages in the sentence\n",
    "ar = [int(i) for i in re.findall('\\d{2,3}', sent)]\n",
    "print(f'Average age : {sum(ar)/len(ar) : .2f}')\n",
    "\n",
    "ar=[int(i) for i in sent.split(\" \") if i.isdigit()]\n",
    "print(f'Average age : {sum(ar)/len(ar) : .2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡§Ö‡§ú‡§Ø\n",
      "‡§Ö‡§§‡•Å‡§≤\n",
      "‡§Ö‡§ú‡§Ø ‡§µ‡§ø‡§ú‡§Ø ‡§™‡•ç‡§∞‡§ø‡§Ø‡§æ ‡§Ö‡§§‡•Å‡§≤\n",
      "‡§Ö‡§ú‡§Ø\n",
      "6\n",
      "‡•Å "
     ]
    }
   ],
   "source": [
    "names = ['‡§Ö‡§ú‡§Ø', '‡§µ‡§ø‡§ú‡§Ø', '‡§™‡•ç‡§∞‡§ø‡§Ø‡§æ', '‡§Ö‡§§‡•Å‡§≤' ] \n",
    "for name in names:\n",
    "    if name.startswith('‡§Ö'):\n",
    "        print(name)\n",
    "\n",
    "print(' '.join(names))\n",
    "\n",
    "print('‡§µ‡§ø‡§ú‡§Ø'.replace('‡§µ‡§ø', '‡§Ö'))\n",
    "\n",
    "print(len(names[2]))\n",
    "\n",
    "for i in name[2]:\n",
    "    print(i, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('names.txt','w', encoding='utf8')\n",
    "for x in range(10):\n",
    "    f.write('‡§™‡•ç‡§∞‡§ø‡§Ø‡§æ\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 2352, 49, '‡§∞', 2351)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('r'),ord('‡§∞'), ord('1'), chr(2352), ord('‡§Ø')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§π‡§™'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = '\\u0939\\u092A'\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65039"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 'üòä‚ù§Ô∏èüòçüëçüòé'\n",
    "ord(x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2346 2325 2379 2337 2366 "
     ]
    }
   ],
   "source": [
    "for x in '‡§™‡§ï‡•ã‡§°‡§æ':\n",
    "    print(ord(x), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    background: linear-gradient(90deg,rgb(251, 255, 10), #ff758c, #ff4d6d);\n",
    "    -webkit-background-clip: text;\n",
    "    -webkit-text-fill-color: transparent;\n",
    "    font-size: 24px;\n",
    "    font-weight: bold;\n",
    "    text-align: center;\">\n",
    "  Tokenization\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall numpy scipy -y\n",
    "# !pip install numpy==1.26.4 scipy==1.11.3\n",
    "## !pip install --force-reinstall numpy==1.26.4 scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DAI.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DAI.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DAI.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\DAI.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\DAI.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package indian to\n",
      "[nltk_data]     C:\\Users\\DAI.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package indian is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\DAI.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DAI.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')                                # tokenizer\n",
    "nltk.download('stopwords')                            # collection of stopwords\n",
    "nltk.download('wordnet')                              # wordnet -> database of english words\n",
    "nltk.download('omw-1.4')                              # open multilingual wordnet\n",
    "nltk.download('averaged_perceptron_tagger')           # POS Tagger\n",
    "nltk.download('indian')                               # indian language POS tagger\n",
    "nltk.download('maxent_ne_chunker')                    # maxent chunking\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    background: linear-gradient(90deg,rgb(251, 255, 10), #ff758c, #ff4d6d);\n",
    "    -webkit-background-clip: text;\n",
    "    -webkit-text-fill-color: transparent;\n",
    "    font-size: 20px;\n",
    "    font-weight: bold;\n",
    "    text-align: center;\">\n",
    "  Word Tokenization\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They', 'told', 'that', 'their', 'ages', 'of', 'mr.', 'are', '25,', '26.2', 'and', '31', 'respectively.']\n",
      "['They', 'told', 'that', 'their', 'ages', 'of', 'mr.', 'are', '25', ',', '26.2', 'and', '31', 'respectively', '.']\n"
     ]
    }
   ],
   "source": [
    "sent = 'They told that their ages of mr. are 25, 26.2 and 31 respectively.'\n",
    "print(sent.split())\n",
    "print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'friends!', 'How', 'are', 'you?', 'Welcome', 'to', 'the', 'world', 'of', 'python', 'programming.']\n",
      "['Hello', 'friends', '!', 'How', 'are', 'you', '?', 'Welcome', 'to', 'the', 'world', 'of', 'python', 'programming', '.']\n",
      "Percentage of punctuation 20.0%\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "sent = '''Hello friends!\n",
    "How are you? Welcome to the world of python programming.'''\n",
    "\n",
    "print(sent.split())\n",
    "print(word_tokenize(sent))\n",
    "\n",
    "ar = [1 if i in punctuation else 0 for i in word_tokenize(sent)]\n",
    "ar = sum(ar)/len(ar)\n",
    "\n",
    "print(f\"Percentage of punctuation {ar*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  background: linear-gradient(90deg,rgb(251, 255, 10), #ff758c, #ff4d6d);\n",
    "  -webkit-background-clip: text;\n",
    "  -webkit-text-fill-color: transparent;\n",
    "  font-size: 20px;\n",
    "  font-weight: bold;\n",
    "  text-align: center;\"\n",
    ">\n",
    "  Other Tokenizers\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‡§õ‡§§‡•ç‡§∞‡§™‡§§‡§ø',\n",
       " '‡§∂‡§ø‡§µ‡§æ‡§ú‡•Ä',\n",
       " '‡§Æ‡§π‡§æ‡§∞‡§æ‡§ú',\n",
       " '‡§µ‡§∏‡•ç‡§§‡•Å',\n",
       " '‡§∏‡§Ç‡§ó‡•ç‡§∞‡§π‡§æ‡§≤‡§Ø',\n",
       " '‡§Æ‡•Å‡§Ç‡§¨‡§à',\n",
       " ',',\n",
       " '(',\n",
       " '‡§™‡•Ç‡§∞‡•ç‡§µ',\n",
       " '‡§®‡§æ‡§Æ',\n",
       " \"'The\",\n",
       " 'Prince',\n",
       " 'of',\n",
       " 'Wales',\n",
       " 'Museum',\n",
       " 'of',\n",
       " 'Western',\n",
       " 'India',\n",
       " \"'\",\n",
       " ')',\n",
       " '‡§Æ‡•Å‡§Æ‡•ç‡§¨‡§à',\n",
       " '‡§ï‡§æ',\n",
       " '‡§Æ‡•Å‡§ñ‡•ç‡§Ø',\n",
       " '‡§∏‡§Ç‡§ó‡•ç‡§∞‡§π‡§æ‡§≤‡§Ø',\n",
       " '‡§π‡•à‡•§',\n",
       " '‡§á‡§∏‡§ï‡§æ',\n",
       " '‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§£',\n",
       " '‡§µ‡•á‡§≤‡•ç‡§∏',\n",
       " '‡§ï‡•á',\n",
       " '‡§∞‡§æ‡§ú‡§ï‡•Å‡§Æ‡§æ‡§∞',\n",
       " '‡§ï‡•á',\n",
       " '‡§≠‡§æ‡§∞‡§§',\n",
       " '‡§Ø‡§æ‡§§‡•ç‡§∞‡§æ',\n",
       " '‡§ï‡•á',\n",
       " '‡§∏‡§Æ‡§Ø',\n",
       " '‡§Æ‡•Å‡§Æ‡•ç‡§¨‡§à',\n",
       " '‡§ï‡•á',\n",
       " '‡§™‡•ç‡§∞‡§§‡§ø‡§∑‡•ç‡§†‡§ø‡§§',\n",
       " '‡§â‡§¶‡•ç‡§Ø‡•ã‡§ó‡§™‡§§‡§ø‡§Ø‡•ã',\n",
       " '‡§î‡§∞',\n",
       " '‡§®‡§æ‡§ó‡§∞‡§ø‡§ï‡•ã‡§Ç',\n",
       " '‡§∏‡•á',\n",
       " '‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§',\n",
       " '‡§∏‡§π‡§æ‡§Ø‡§§‡§æ',\n",
       " '‡§Æ‡•Å‡§Æ‡•ç‡§¨‡§à',\n",
       " '‡§ï‡•á',\n",
       " '‡§∏‡§∞‡§ï‡§æ‡§∞',\n",
       " '‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ',\n",
       " '‡§∏‡•ç‡§Æ‡§æ‡§∞‡§ï',\n",
       " '‡§ï‡•á',\n",
       " '‡§∞‡•Ç‡§™',\n",
       " '‡§Æ‡•á‡§Ç',\n",
       " '‡§®‡§ø‡§∞‡•ç‡§Æ‡§ø‡§§',\n",
       " '‡§ï‡§ø‡§Ø‡§æ',\n",
       " '‡§ó‡§Ø‡§æ',\n",
       " '‡§•‡§æ‡•§',\n",
       " '‡§Ø‡§π',\n",
       " '‡§≠‡§µ‡•ç‡§Ø',\n",
       " '‡§≠‡§µ‡§®',\n",
       " '‡§¶‡§ï‡•ç‡§∑‡§ø‡§£',\n",
       " '‡§Æ‡•Å‡§Æ‡•ç‡§¨‡§à',\n",
       " '‡§ï‡•á',\n",
       " '‡§´‡•ã‡§∞‡•ç‡§ü',\n",
       " '‡§µ‡§ø‡§≤‡§ø‡§Ø‡§Æ',\n",
       " '‡§Æ‡•á',\n",
       " ',',\n",
       " '‡§è‡§≤‡•ç‡§´‡§ø‡§Ç‡§∏‡•ç‡§ü‡§®',\n",
       " '‡§ï‡§æ‡§≤‡•á‡§ú',\n",
       " '‡§ï‡•á',\n",
       " '‡§∏‡§æ‡§Æ‡§®‡•á',\n",
       " '‡§π‡•à‡•§',\n",
       " '‡§á‡§∏‡§ï‡•á',\n",
       " '‡§∏‡§æ‡§Æ‡§®‡•á',\n",
       " '‡§∞‡•Ä‡§ó‡§≤',\n",
       " '‡§∏‡§ø‡§®‡§ø‡§Æ‡§æ',\n",
       " '‡§î‡§∞',\n",
       " '‡§™‡•Å‡§≤‡§ø‡§∏',\n",
       " '‡§Ü‡§Ø‡•Å‡§ï‡•ç‡§§',\n",
       " '‡§ï‡§æ‡§∞‡•ç‡§Ø‡§æ‡§≤‡§Ø',\n",
       " '‡§∏‡•ç‡§•‡§ø‡§§',\n",
       " '‡§π‡•à‡•§',\n",
       " '‡§¨‡§ó‡§≤',\n",
       " '‡§Æ‡•á',\n",
       " \"'\",\n",
       " '‡§°‡•Ö‡§µ‡§ø‡§°',\n",
       " '‡§∏‡§∏‡•Ç‡§®',\n",
       " '‡§™‡•Å‡§∏‡•ç‡§§‡§ï',\n",
       " '‡§∏‡§Ç‡§ó‡•ç‡§∞‡§π‡§æ‡§≤‡§Ø',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " '‡§µ‡•ç‡§Ø‡§æ‡§ü‡•ç‡§∏‡•ç‡§®',\n",
       " '‡§π‡•â‡§ü‡•á‡§≤‡•ç',\n",
       " \"'\",\n",
       " ',',\n",
       " '‡§≠‡•Ä',\n",
       " '‡§π‡•à‡•§',\n",
       " '‡§Ü‡§ó‡•á',\n",
       " '‡§∏‡§Æ‡•Å‡§¶‡•ç‡§∞',\n",
       " '‡§ï‡•á',\n",
       " '‡§§‡§∞‡§´',\n",
       " '‡§ú‡§æ‡§®‡•á',\n",
       " '‡§™‡§∞',\n",
       " \"'\",\n",
       " '‡§ó‡•á‡§ü‡§µ‡•á',\n",
       " '‡§ë‡§´‡§º',\n",
       " '‡§á‡§®‡•ç‡§°‡§ø‡§Ø‡§æ',\n",
       " \"'\",\n",
       " '‡§Ü‡§§‡§æ',\n",
       " '‡§π‡•à‡•§']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = '''\n",
    "‡§õ‡§§‡•ç‡§∞‡§™‡§§‡§ø ‡§∂‡§ø‡§µ‡§æ‡§ú‡•Ä ‡§Æ‡§π‡§æ‡§∞‡§æ‡§ú ‡§µ‡§∏‡•ç‡§§‡•Å ‡§∏‡§Ç‡§ó‡•ç‡§∞‡§π‡§æ‡§≤‡§Ø ‡§Æ‡•Å‡§Ç‡§¨‡§à, (‡§™‡•Ç‡§∞‡•ç‡§µ ‡§®‡§æ‡§Æ 'The Prince of Wales Museum of Western India') ‡§Æ‡•Å‡§Æ‡•ç‡§¨‡§à ‡§ï‡§æ ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§∏‡§Ç‡§ó‡•ç‡§∞‡§π‡§æ‡§≤‡§Ø ‡§π‡•à‡•§ \n",
    "‡§á‡§∏‡§ï‡§æ ‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§£ ‡§µ‡•á‡§≤‡•ç‡§∏ ‡§ï‡•á ‡§∞‡§æ‡§ú‡§ï‡•Å‡§Æ‡§æ‡§∞ ‡§ï‡•á ‡§≠‡§æ‡§∞‡§§ ‡§Ø‡§æ‡§§‡•ç‡§∞‡§æ ‡§ï‡•á ‡§∏‡§Æ‡§Ø ‡§Æ‡•Å‡§Æ‡•ç‡§¨‡§à ‡§ï‡•á ‡§™‡•ç‡§∞‡§§‡§ø‡§∑‡•ç‡§†‡§ø‡§§ ‡§â‡§¶‡•ç‡§Ø‡•ã‡§ó‡§™‡§§‡§ø‡§Ø‡•ã ‡§î‡§∞ ‡§®‡§æ‡§ó‡§∞‡§ø‡§ï‡•ã‡§Ç ‡§∏‡•á ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§∏‡§π‡§æ‡§Ø‡§§‡§æ ‡§Æ‡•Å‡§Æ‡•ç‡§¨‡§à ‡§ï‡•á ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§∏‡•ç‡§Æ‡§æ‡§∞‡§ï ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§®‡§ø‡§∞‡•ç‡§Æ‡§ø‡§§ ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§•‡§æ‡•§ \n",
    "‡§Ø‡§π ‡§≠‡§µ‡•ç‡§Ø ‡§≠‡§µ‡§® ‡§¶‡§ï‡•ç‡§∑‡§ø‡§£ ‡§Æ‡•Å‡§Æ‡•ç‡§¨‡§à ‡§ï‡•á ‡§´‡•ã‡§∞‡•ç‡§ü ‡§µ‡§ø‡§≤‡§ø‡§Ø‡§Æ ‡§Æ‡•á, ‡§è‡§≤‡•ç‡§´‡§ø‡§Ç‡§∏‡•ç‡§ü‡§® ‡§ï‡§æ‡§≤‡•á‡§ú ‡§ï‡•á ‡§∏‡§æ‡§Æ‡§®‡•á ‡§π‡•à‡•§ \n",
    "‡§á‡§∏‡§ï‡•á ‡§∏‡§æ‡§Æ‡§®‡•á ‡§∞‡•Ä‡§ó‡§≤ ‡§∏‡§ø‡§®‡§ø‡§Æ‡§æ ‡§î‡§∞ ‡§™‡•Å‡§≤‡§ø‡§∏ ‡§Ü‡§Ø‡•Å‡§ï‡•ç‡§§ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§æ‡§≤‡§Ø ‡§∏‡•ç‡§•‡§ø‡§§ ‡§π‡•à‡•§\n",
    "‡§¨‡§ó‡§≤ ‡§Æ‡•á '‡§°‡•Ö‡§µ‡§ø‡§° ‡§∏‡§∏‡•Ç‡§® ‡§™‡•Å‡§∏‡•ç‡§§‡§ï ‡§∏‡§Ç‡§ó‡•ç‡§∞‡§π‡§æ‡§≤‡§Ø', '‡§µ‡•ç‡§Ø‡§æ‡§ü‡•ç‡§∏‡•ç‡§® ‡§π‡•â‡§ü‡•á‡§≤‡•ç', ‡§≠‡•Ä ‡§π‡•à‡•§ \n",
    "‡§Ü‡§ó‡•á ‡§∏‡§Æ‡•Å‡§¶‡•ç‡§∞ ‡§ï‡•á ‡§§‡§∞‡§´ ‡§ú‡§æ‡§®‡•á ‡§™‡§∞ '‡§ó‡•á‡§ü‡§µ‡•á ‡§ë‡§´‡§º ‡§á‡§®‡•ç‡§°‡§ø‡§Ø‡§æ' ‡§Ü‡§§‡§æ ‡§π‡•à‡•§\n",
    "'''\n",
    "\n",
    "word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['„É†„É≥„Éê„Ç§„ÅÆ„ÉÅ„É£„Éà„É©„Éë„ÉÜ„Ç£',\n",
       " '„Ç∑„É¥„Ç°„Éº„Ç∏„Éº',\n",
       " '„Éû„Éè„É©„Ç∏ÂçöÁâ©È§®',\n",
       " '(',\n",
       " '‰ª•Ââç„ÅØ„ÄåË•ø„Ç§„É≥„Éâ„ÅÆ„Éó„É™„É≥„Çπ',\n",
       " '„Ç™„Éñ',\n",
       " '„Ç¶„Çß„Éº„É´„Ç∫ÂçöÁâ©È§®„Äç„Å®„Åó„Å¶Áü•„Çâ„Çå„Å¶„ÅÑ„Åæ„Åó„Åü',\n",
       " ')',\n",
       " '„ÅØ„ÄÅ„É†„É≥„Éê„Ç§„ÅÆ‰∏ªË¶Å„Å™ÂçöÁâ©È§®„Åß„Åô„ÄÇ',\n",
       " '„Åì„ÅÆË®òÂøµÁ¢ë„ÅØ„ÄÅ„Ç¶„Çß„Éº„É´„Ç∫ÁöáÂ§™Â≠ê„ÅÆ„Ç§„É≥„ÉâË®™Âïè‰∏≠„Å´„É†„É≥„Éê„Ç§ÊîøÂ∫ú„ÅåËëóÂêç„Å™ÂÆüÊ•≠ÂÆ∂„ÇÑ„É†„É≥„Éê„Ç§Â∏ÇÊ∞ë„ÅÆÂçîÂäõ„ÇíÂæó„Å¶Ë®òÂøµÁ¢ë„Å®„Åó„Å¶Âª∫„Å¶„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ',\n",
       " '„Åì„ÅÆÂ£ÆÂ§ß„Å™Âª∫Áâ©„ÅØ„ÄÅÂçó„É†„É≥„Éê„Ç§„ÅÆ„Éï„Ç©„Éº„Éà',\n",
       " '„Ç¶„Ç£„É™„Ç¢„É†„ÄÅ„Ç®„É´„Éï„Ç£„É≥„Çπ„Éà„Éº„É≥Â§ßÂ≠¶„ÅÆÂêë„Åã„ÅÑ„Å´„ÅÇ„Çä„Åæ„Åô„ÄÇ',\n",
       " '„É™„Éº„Ç¨„É´„Ç∑„Éç„Éû„Å®Ë≠¶ÂØüÂ∫ÅËàé„ÅåÁõÆ„ÅÆÂâç„Å´„ÅÇ„Çä„Åæ„Åô„ÄÇ',\n",
       " 'Ëøë„Åè„Å´„ÅØ„Äå„Éá„É¥„Ç£„ÉÉ„Éâ„Éª„Çµ„Çπ„Éº„É≥„Éª„Éñ„ÉÉ„ÇØ„Éª„Éü„É•„Éº„Ç∏„Ç¢„É†„Äç„ÄÅ„Äå„É¥„É£„Éà„É≥„Éª„Éõ„ÉÜ„É´„Äç„ÇÇ„ÅÇ„Çä„Åæ„Åô„ÄÇ',\n",
       " '„Åï„Çâ„Å´Êµ∑„ÅÆ„Åª„ÅÜ„Å∏Ë°å„Åè„Å®„Äå„Ç§„É≥„ÉâÈñÄ„Äç„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"\"\"\n",
    "„É†„É≥„Éê„Ç§„ÅÆ„ÉÅ„É£„Éà„É©„Éë„ÉÜ„Ç£ „Ç∑„É¥„Ç°„Éº„Ç∏„Éº „Éû„Éè„É©„Ç∏ÂçöÁâ©È§® (‰ª•Ââç„ÅØ„ÄåË•ø„Ç§„É≥„Éâ„ÅÆ„Éó„É™„É≥„Çπ „Ç™„Éñ „Ç¶„Çß„Éº„É´„Ç∫ÂçöÁâ©È§®„Äç„Å®„Åó„Å¶Áü•„Çâ„Çå„Å¶„ÅÑ„Åæ„Åó„Åü) „ÅØ„ÄÅ„É†„É≥„Éê„Ç§„ÅÆ‰∏ªË¶Å„Å™ÂçöÁâ©È§®„Åß„Åô„ÄÇ\n",
    "„Åì„ÅÆË®òÂøµÁ¢ë„ÅØ„ÄÅ„Ç¶„Çß„Éº„É´„Ç∫ÁöáÂ§™Â≠ê„ÅÆ„Ç§„É≥„ÉâË®™Âïè‰∏≠„Å´„É†„É≥„Éê„Ç§ÊîøÂ∫ú„ÅåËëóÂêç„Å™ÂÆüÊ•≠ÂÆ∂„ÇÑ„É†„É≥„Éê„Ç§Â∏ÇÊ∞ë„ÅÆÂçîÂäõ„ÇíÂæó„Å¶Ë®òÂøµÁ¢ë„Å®„Åó„Å¶Âª∫„Å¶„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ\n",
    "„Åì„ÅÆÂ£ÆÂ§ß„Å™Âª∫Áâ©„ÅØ„ÄÅÂçó„É†„É≥„Éê„Ç§„ÅÆ„Éï„Ç©„Éº„Éà „Ç¶„Ç£„É™„Ç¢„É†„ÄÅ„Ç®„É´„Éï„Ç£„É≥„Çπ„Éà„Éº„É≥Â§ßÂ≠¶„ÅÆÂêë„Åã„ÅÑ„Å´„ÅÇ„Çä„Åæ„Åô„ÄÇ\n",
    "„É™„Éº„Ç¨„É´„Ç∑„Éç„Éû„Å®Ë≠¶ÂØüÂ∫ÅËàé„ÅåÁõÆ„ÅÆÂâç„Å´„ÅÇ„Çä„Åæ„Åô„ÄÇ\n",
    "Ëøë„Åè„Å´„ÅØ„Äå„Éá„É¥„Ç£„ÉÉ„Éâ„Éª„Çµ„Çπ„Éº„É≥„Éª„Éñ„ÉÉ„ÇØ„Éª„Éü„É•„Éº„Ç∏„Ç¢„É†„Äç„ÄÅ„Äå„É¥„É£„Éà„É≥„Éª„Éõ„ÉÜ„É´„Äç„ÇÇ„ÅÇ„Çä„Åæ„Åô„ÄÇ\n",
    "„Åï„Çâ„Å´Êµ∑„ÅÆ„Åª„ÅÜ„Å∏Ë°å„Åè„Å®„Äå„Ç§„É≥„ÉâÈñÄ„Äç„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ\"\"\"\n",
    "\n",
    "word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  background: linear-gradient(90deg,rgb(251, 255, 10), #ff758c, #ff4d6d);\n",
    "  -webkit-background-clip: text;\n",
    "  -webkit-text-fill-color: transparent;\n",
    "  font-size: 20px;\n",
    "  font-weight: bold;\n",
    "  text-align: center;\"\n",
    ">\n",
    "  Senetence Tokenizers\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends!',\n",
       " 'How are you?',\n",
       " 'Welcome to the world of python programming.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = '''Hello friends!\n",
    "How are you? Welcome to the world of python programming.'''\n",
    "\n",
    "sent_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\n‡§õ‡§§‡•ç‡§∞‡§™‡§§‡§ø ‡§∂‡§ø‡§µ‡§æ‡§ú‡•Ä ‡§Æ‡§π‡§æ‡§∞‡§æ‡§ú ‡§µ‡§∏‡•ç‡§§‡•Å ‡§∏‡§Ç‡§ó‡•ç‡§∞‡§π‡§æ‡§≤‡§Ø ‡§Æ‡•Å‡§Ç‡§¨‡§à, (‡§™‡•Ç‡§∞‡•ç‡§µ ‡§®‡§æ‡§Æ 'The Prince of Wales Museum of Western India') ‡§Æ‡•Å‡§Æ‡•ç‡§¨‡§à ‡§ï‡§æ ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§∏‡§Ç‡§ó‡•ç‡§∞‡§π‡§æ‡§≤‡§Ø ‡§π‡•à.\",\n",
       " '‡§á‡§∏‡§ï‡§æ ‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§£ ‡§µ‡•á‡§≤‡•ç‡§∏ ‡§ï‡•á ‡§∞‡§æ‡§ú‡§ï‡•Å‡§Æ‡§æ‡§∞ ‡§ï‡•á ‡§≠‡§æ‡§∞‡§§ ‡§Ø‡§æ‡§§‡•ç‡§∞‡§æ ‡§ï‡•á ‡§∏‡§Æ‡§Ø ‡§Æ‡•Å‡§Æ‡•ç‡§¨‡§à ‡§ï‡•á ‡§™‡•ç‡§∞‡§§‡§ø‡§∑‡•ç‡§†‡§ø‡§§ ‡§â‡§¶‡•ç‡§Ø‡•ã‡§ó‡§™‡§§‡§ø‡§Ø‡•ã ‡§î‡§∞ ‡§®‡§æ‡§ó‡§∞‡§ø‡§ï‡•ã‡§Ç ‡§∏‡•á ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§∏‡§π‡§æ‡§Ø‡§§‡§æ ‡§Æ‡•Å‡§Æ‡•ç‡§¨‡§à ‡§ï‡•á ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§∏‡•ç‡§Æ‡§æ‡§∞‡§ï ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§®‡§ø‡§∞‡•ç‡§Æ‡§ø‡§§ ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§•‡§æ.',\n",
       " '‡§Ø‡§π ‡§≠‡§µ‡•ç‡§Ø ‡§≠‡§µ‡§® ‡§¶‡§ï‡•ç‡§∑‡§ø‡§£ ‡§Æ‡•Å‡§Æ‡•ç‡§¨‡§à ‡§ï‡•á ‡§´‡•ã‡§∞‡•ç‡§ü ‡§µ‡§ø‡§≤‡§ø‡§Ø‡§Æ ‡§Æ‡•á, ‡§è‡§≤‡•ç‡§´‡§ø‡§Ç‡§∏‡•ç‡§ü‡§® ‡§ï‡§æ‡§≤‡•á‡§ú ‡§ï‡•á ‡§∏‡§æ‡§Æ‡§®‡•á ‡§π‡•à.',\n",
       " '‡§á‡§∏‡§ï‡•á ‡§∏‡§æ‡§Æ‡§®‡•á ‡§∞‡•Ä‡§ó‡§≤ ‡§∏‡§ø‡§®‡§ø‡§Æ‡§æ ‡§î‡§∞ ‡§™‡•Å‡§≤‡§ø‡§∏ ‡§Ü‡§Ø‡•Å‡§ï‡•ç‡§§ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§æ‡§≤‡§Ø ‡§∏‡•ç‡§•‡§ø‡§§ ‡§π‡•à.',\n",
       " \"‡§¨‡§ó‡§≤ ‡§Æ‡•á '‡§°‡•Ö‡§µ‡§ø‡§° ‡§∏‡§∏‡•Ç‡§® ‡§™‡•Å‡§∏‡•ç‡§§‡§ï ‡§∏‡§Ç‡§ó‡•ç‡§∞‡§π‡§æ‡§≤‡§Ø', '‡§µ‡•ç‡§Ø‡§æ‡§ü‡•ç‡§∏‡•ç‡§® ‡§π‡•â‡§ü‡•á‡§≤‡•ç', ‡§≠‡•Ä ‡§π‡•à.\",\n",
       " \"‡§Ü‡§ó‡•á ‡§∏‡§Æ‡•Å‡§¶‡•ç‡§∞ ‡§ï‡•á ‡§§‡§∞‡§´ ‡§ú‡§æ‡§®‡•á ‡§™‡§∞ '‡§ó‡•á‡§ü‡§µ‡•á ‡§ë‡§´‡§º ‡§á‡§®‡•ç‡§°‡§ø‡§Ø‡§æ' ‡§Ü‡§§‡§æ ‡§π‡•à.\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = '''\n",
    "‡§õ‡§§‡•ç‡§∞‡§™‡§§‡§ø ‡§∂‡§ø‡§µ‡§æ‡§ú‡•Ä ‡§Æ‡§π‡§æ‡§∞‡§æ‡§ú ‡§µ‡§∏‡•ç‡§§‡•Å ‡§∏‡§Ç‡§ó‡•ç‡§∞‡§π‡§æ‡§≤‡§Ø ‡§Æ‡•Å‡§Ç‡§¨‡§à, (‡§™‡•Ç‡§∞‡•ç‡§µ ‡§®‡§æ‡§Æ 'The Prince of Wales Museum of Western India') ‡§Æ‡•Å‡§Æ‡•ç‡§¨‡§à ‡§ï‡§æ ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§∏‡§Ç‡§ó‡•ç‡§∞‡§π‡§æ‡§≤‡§Ø ‡§π‡•à.\n",
    "‡§á‡§∏‡§ï‡§æ ‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§£ ‡§µ‡•á‡§≤‡•ç‡§∏ ‡§ï‡•á ‡§∞‡§æ‡§ú‡§ï‡•Å‡§Æ‡§æ‡§∞ ‡§ï‡•á ‡§≠‡§æ‡§∞‡§§ ‡§Ø‡§æ‡§§‡•ç‡§∞‡§æ ‡§ï‡•á ‡§∏‡§Æ‡§Ø ‡§Æ‡•Å‡§Æ‡•ç‡§¨‡§à ‡§ï‡•á ‡§™‡•ç‡§∞‡§§‡§ø‡§∑‡•ç‡§†‡§ø‡§§ ‡§â‡§¶‡•ç‡§Ø‡•ã‡§ó‡§™‡§§‡§ø‡§Ø‡•ã ‡§î‡§∞ ‡§®‡§æ‡§ó‡§∞‡§ø‡§ï‡•ã‡§Ç ‡§∏‡•á ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§∏‡§π‡§æ‡§Ø‡§§‡§æ ‡§Æ‡•Å‡§Æ‡•ç‡§¨‡§à ‡§ï‡•á ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§∏‡•ç‡§Æ‡§æ‡§∞‡§ï ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§®‡§ø‡§∞‡•ç‡§Æ‡§ø‡§§ ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§•‡§æ.\n",
    "‡§Ø‡§π ‡§≠‡§µ‡•ç‡§Ø ‡§≠‡§µ‡§® ‡§¶‡§ï‡•ç‡§∑‡§ø‡§£ ‡§Æ‡•Å‡§Æ‡•ç‡§¨‡§à ‡§ï‡•á ‡§´‡•ã‡§∞‡•ç‡§ü ‡§µ‡§ø‡§≤‡§ø‡§Ø‡§Æ ‡§Æ‡•á, ‡§è‡§≤‡•ç‡§´‡§ø‡§Ç‡§∏‡•ç‡§ü‡§® ‡§ï‡§æ‡§≤‡•á‡§ú ‡§ï‡•á ‡§∏‡§æ‡§Æ‡§®‡•á ‡§π‡•à.\n",
    "‡§á‡§∏‡§ï‡•á ‡§∏‡§æ‡§Æ‡§®‡•á ‡§∞‡•Ä‡§ó‡§≤ ‡§∏‡§ø‡§®‡§ø‡§Æ‡§æ ‡§î‡§∞ ‡§™‡•Å‡§≤‡§ø‡§∏ ‡§Ü‡§Ø‡•Å‡§ï‡•ç‡§§ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§æ‡§≤‡§Ø ‡§∏‡•ç‡§•‡§ø‡§§ ‡§π‡•à.\n",
    "‡§¨‡§ó‡§≤ ‡§Æ‡•á '‡§°‡•Ö‡§µ‡§ø‡§° ‡§∏‡§∏‡•Ç‡§® ‡§™‡•Å‡§∏‡•ç‡§§‡§ï ‡§∏‡§Ç‡§ó‡•ç‡§∞‡§π‡§æ‡§≤‡§Ø', '‡§µ‡•ç‡§Ø‡§æ‡§ü‡•ç‡§∏‡•ç‡§® ‡§π‡•â‡§ü‡•á‡§≤‡•ç', ‡§≠‡•Ä ‡§π‡•à. \n",
    "‡§Ü‡§ó‡•á ‡§∏‡§Æ‡•Å‡§¶‡•ç‡§∞ ‡§ï‡•á ‡§§‡§∞‡§´ ‡§ú‡§æ‡§®‡•á ‡§™‡§∞ '‡§ó‡•á‡§ü‡§µ‡•á ‡§ë‡§´‡§º ‡§á‡§®‡•ç‡§°‡§ø‡§Ø‡§æ' ‡§Ü‡§§‡§æ ‡§π‡•à.\n",
    "'''\n",
    "\n",
    "sent_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the max sentence which has the most the word 'the'\n",
    "sent = \"\"\"India, officially the Republic of India,[j][20] is a country in South Asia. It is the seventh-largest country in the world by area and the most populous country. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west;[k] China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand, Myanmar, and Indonesia.\n",
    "\n",
    "Modern humans arrived on the Indian subcontinent from Africa no later than 55,000 years ago.[22][23][24] Their long occupation, initially in varying forms of isolation as hunter-gatherers, has made the region highly diverse, second only to Africa in human genetic diversity.[25] Settled life emerged on the subcontinent in the western margins of the Indus river basin 9,000 years ago, evolving gradually into the Indus Valley Civilisation of the third millennium BCE.[26] By at least 1200 BCE, an archaic form of Sanskrit, an Indo-European language, had diffused into India from the northwest.[27][28] Its evidence today is found in the hymns of the Rigveda. Preserved by an oral tradition that was resolutely vigilant, the Rigveda records the dawning of Hinduism in India.[29] The Dravidian languages of India were supplanted in the northern and western regions.[30] By 400 BCE, stratification and exclusion by caste had emerged within Hinduism,[31] and Buddhism and Jainism had arisen, proclaiming social orders unlinked to heredity.[32] Early political consolidations gave rise to the loose-knit Maurya and Gupta Empires based in the Ganges Basin.[33] Their collective era was suffused with wide-ranging creativity,[34] but also marked by the declining status of women,[35] and the incorporation of untouchability into an organised system of belief.[l][36] The Middle kingdoms exported Sanskrit language, south Indian scripts and religions of Hinduism and Buddhism to the Southeast Asia.\n",
    "\"\"\"\n",
    "sents = sent_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west;[k] China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = {i : j.lower().count('the')  for i,j in enumerate(sents)}\n",
    "\n",
    "idex = sorted(res.items(), key=lambda x: x[1], reverse=True)\n",
    "display(idex[0][1], sents[idex[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west;[k] China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(\n",
    "    lambda x, y : x if (x.lower().count('the') > y.lower().count('the')) else y,\n",
    "    sents\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  background: linear-gradient(90deg,rgb(251, 255, 10), #ff758c, #ff4d6d);\n",
    "  -webkit-background-clip: text;\n",
    "  -webkit-text-fill-color: transparent;\n",
    "  font-size: 20px;\n",
    "  font-weight: bold;\n",
    "  text-align: center;\"\n",
    ">\n",
    "  Whitespace Tokenizers\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'python',\n",
       " 'programming.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent = '''Hello friends!\n",
    "How are you? Welcome to the world of python programming.'''\n",
    "tk = WhitespaceTokenizer()\n",
    "\n",
    "\n",
    "display(tk.tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  background: linear-gradient(90deg,rgb(251, 255, 10), #ff758c, #ff4d6d);\n",
    "  -webkit-background-clip: text;\n",
    "  -webkit-text-fill-color: transparent;\n",
    "  font-size: 20px;\n",
    "  font-weight: bold;\n",
    "  text-align: center;\"\n",
    ">\n",
    "Space Tokenizers\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import SpaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends!\\nHow',\n",
       " 'are',\n",
       " 'you?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'python',\n",
       " 'programming.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends!\\nHow',\n",
       " 'are',\n",
       " 'you?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'python',\n",
       " 'programming.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent = '''Hello friends!\n",
    "How are you? Welcome to the world of python programming.'''\n",
    "\n",
    "\n",
    "# Only take spaces not the newline\n",
    "tk = SpaceTokenizer()\n",
    "\n",
    "display(sent.split(' '))\n",
    "display(tk.tokenize(sent)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  background: linear-gradient(90deg,rgb(251, 255, 10), #ff758c, #ff4d6d);\n",
    "  -webkit-background-clip: text;\n",
    "  -webkit-text-fill-color: transparent;\n",
    "  font-size: 20px;\n",
    "  font-weight: bold;\n",
    "  text-align: center;\"\n",
    ">\n",
    "Line Tokenizers\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import LineTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends!', 'How are you? Welcome to the world of python programming.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Hello friends!', 'How are you? Welcome to the world of python programming.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent = '''Hello friends!\n",
    "How are you? Welcome to the world of python programming.'''\n",
    "\n",
    "\n",
    "# Only take newline\n",
    "tk = LineTokenizer()\n",
    "\n",
    "display(sent.split('\\n'))\n",
    "display(tk.tokenize(sent)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  background: linear-gradient(90deg,rgb(251, 255, 10), #ff758c, #ff4d6d);\n",
    "  -webkit-background-clip: text;\n",
    "  -webkit-text-fill-color: transparent;\n",
    "  font-size: 20px;\n",
    "  font-weight: bold;\n",
    "  text-align: center;\"\n",
    ">\n",
    "Tab Tokenizers\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TabTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends!\\nHow are you? Welcome to',\n",
       " 'the world of python',\n",
       " 'programming.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Hello friends!\\nHow are you? Welcome to',\n",
       " 'the world of python',\n",
       " 'programming.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent = '''Hello friends!\n",
    "How are you? Welcome to\\tthe world of python\\tprogramming.'''\n",
    "\n",
    "\n",
    "# Only take tab\n",
    "tk = TabTokenizer()\n",
    "\n",
    "display(sent.split('\\t'))\n",
    "display(tk.tokenize(sent)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  background: linear-gradient(90deg,rgb(251, 255, 10), #ff758c, #ff4d6d);\n",
    "  -webkit-background-clip: text;\n",
    "  -webkit-text-fill-color: transparent;\n",
    "  font-size: 20px;\n",
    "  font-weight: bold;\n",
    "  text-align: center;\"\n",
    ">\n",
    "Tweet Tokenizers\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'ü´£',\n",
       " 'friends',\n",
       " '!',\n",
       " ':',\n",
       " ')',\n",
       " 'üíÄ',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " ':',\n",
       " '$',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'pythonüêç',\n",
       " 'programming.',\n",
       " '<',\n",
       " '3']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'ü´£',\n",
       " 'friends',\n",
       " '!',\n",
       " ':)',\n",
       " 'üíÄ',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " ':',\n",
       " '$',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'python',\n",
       " 'üêç',\n",
       " 'programming',\n",
       " '.',\n",
       " '<3']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent = '''Hello ü´£ friends!:)üíÄ\n",
    "How are you? Welcome :$ to\\tthe world of pythonüêç\\tprogramming.<3'''\n",
    "\n",
    "tk = TweetTokenizer()\n",
    "# It tokenizes based on the emojis and emoji generator keycombinations( :) smile, :( sad face, <3 heart, etc )\n",
    " \n",
    "display(word_tokenize(sent))\n",
    "display(tk.tokenize(sent)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  background: linear-gradient(90deg,rgb(251, 255, 10), #ff758c, #ff4d6d);\n",
    "  -webkit-background-clip: text;\n",
    "  -webkit-text-fill-color: transparent;\n",
    "  font-size: 20px;\n",
    "  font-weight: bold;\n",
    "  text-align: center;\"\n",
    ">\n",
    "Multi Word Extension Tokenizers\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Van Rossom',\n",
       " 'is',\n",
       " 'in',\n",
       " 'Pune',\n",
       " '.',\n",
       " 'We',\n",
       " 'welcomed',\n",
       " 'Van Rossom',\n",
       " 'here',\n",
       " '.',\n",
       " 'Van',\n",
       " 'Nayak',\n",
       " 'is',\n",
       " 'also',\n",
       " 'in',\n",
       " 'pune',\n",
       " 'doing',\n",
       " 'Majdoori',\n",
       " 'in',\n",
       " 'CDAC']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'Van Rossom is in Pune. We welcomed Van Rossom here. Van Nayak is also in pune doing Majdoori in CDAC'\n",
    "\n",
    "# A tokenizer that processes tokenized text and merges multi-word expressions into single tokens.\n",
    "tk = MWETokenizer(separator=' ') \n",
    "\n",
    "tk.add_mwe(('Van', 'Rossom'))\n",
    "tk.tokenize(word_tokenize(sent)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  background: linear-gradient(90deg,rgb(251, 255, 10), #ff758c, #ff4d6d);\n",
    "  -webkit-background-clip: text;\n",
    "  -webkit-text-fill-color: transparent;\n",
    "  font-size: 20px;\n",
    "  font-weight: bold;\n",
    "  text-align: center;\"\n",
    ">\n",
    "Custom Tokenizers\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Van',\n",
       " 'Rossom',\n",
       " 'is',\n",
       " 'in',\n",
       " 'Pune',\n",
       " 'We',\n",
       " 'welcomed',\n",
       " 'Van',\n",
       " 'Rossom',\n",
       " 'here',\n",
       " 'Van',\n",
       " 'Nayak',\n",
       " 'in',\n",
       " '>',\n",
       " 'pune',\n",
       " '&',\n",
       " 'doing',\n",
       " 'Majdoori',\n",
       " 'in',\n",
       " 'CDAC']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent = 'Van Rossom is in Pune. We welcomed Van Rossom here!. Van Nayak in > pune & doing Majdoori in CDAC'\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return re.split(r\"[.,;?!\\s]+\", text)\n",
    "\n",
    "tokens = custom_tokenizer(sent)\n",
    "display(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
