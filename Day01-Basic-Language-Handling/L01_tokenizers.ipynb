{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    background: linear-gradient(90deg,rgb(251, 255, 10), #ff758c, #ff4d6d);\n",
    "    -webkit-background-clip: text;\n",
    "    -webkit-text-fill-color: transparent;\n",
    "    font-size: 20px;\n",
    "    font-weight: bold;\n",
    "    text-align: center;\">\n",
    "  Basic Python\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average age :  27.33\n",
      "Average age :  27.33\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sent = 'They told that their ages are 25 26 and 31 respectively.'\n",
    "\n",
    "#Find the average ages in the sentence\n",
    "ar = [int(i) for i in re.findall('\\d{2,3}', sent)]\n",
    "print(f'Average age : {sum(ar)/len(ar) : .2f}')\n",
    "\n",
    "ar=[int(i) for i in sent.split(\" \") if i.isdigit()]\n",
    "print(f'Average age : {sum(ar)/len(ar) : .2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "рдЕрдЬрдп\n",
      "рдЕрддреБрд▓\n",
      "рдЕрдЬрдп рд╡рд┐рдЬрдп рдкреНрд░рд┐рдпрд╛ рдЕрддреБрд▓\n",
      "рдЕрдЬрдп\n",
      "6\n",
      "реБ "
     ]
    }
   ],
   "source": [
    "names = ['рдЕрдЬрдп', 'рд╡рд┐рдЬрдп', 'рдкреНрд░рд┐рдпрд╛', 'рдЕрддреБрд▓' ] \n",
    "for name in names:\n",
    "    if name.startswith('рдЕ'):\n",
    "        print(name)\n",
    "\n",
    "print(' '.join(names))\n",
    "\n",
    "print('рд╡рд┐рдЬрдп'.replace('рд╡рд┐', 'рдЕ'))\n",
    "\n",
    "print(len(names[2]))\n",
    "\n",
    "for i in name[2]:\n",
    "    print(i, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('names.txt','w', encoding='utf8')\n",
    "for x in range(10):\n",
    "    f.write('рдкреНрд░рд┐рдпрд╛\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 2352, 49, 'рд░', 2351)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('r'),ord('рд░'), ord('1'), chr(2352), ord('рдп')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'рд╣рдк'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = '\\u0939\\u092A'\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65039"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 'ЁЯШКтЭдя╕ПЁЯШНЁЯСНЁЯШО'\n",
    "ord(x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2346 2325 2379 2337 2366 "
     ]
    }
   ],
   "source": [
    "for x in 'рдкрдХреЛрдбрд╛':\n",
    "    print(ord(x), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    background: linear-gradient(90deg,rgb(251, 255, 10), #ff758c, #ff4d6d);\n",
    "    -webkit-background-clip: text;\n",
    "    -webkit-text-fill-color: transparent;\n",
    "    font-size: 24px;\n",
    "    font-weight: bold;\n",
    "    text-align: center;\">\n",
    "  Tokenization\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall numpy scipy -y\n",
    "# !pip install numpy==1.26.4 scipy==1.11.3\n",
    "## !pip install --force-reinstall numpy==1.26.4 scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DAI.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DAI.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DAI.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\DAI.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\DAI.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package indian to\n",
      "[nltk_data]     C:\\Users\\DAI.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package indian is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\DAI.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DAI.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')                                # tokenizer\n",
    "nltk.download('stopwords')                            # collection of stopwords\n",
    "nltk.download('wordnet')                              # wordnet -> database of english words\n",
    "nltk.download('omw-1.4')                              # open multilingual wordnet\n",
    "nltk.download('averaged_perceptron_tagger')           # POS Tagger\n",
    "nltk.download('indian')                               # indian language POS tagger\n",
    "nltk.download('maxent_ne_chunker')                    # maxent chunking\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    background: linear-gradient(90deg,rgb(251, 255, 10), #ff758c, #ff4d6d);\n",
    "    -webkit-background-clip: text;\n",
    "    -webkit-text-fill-color: transparent;\n",
    "    font-size: 20px;\n",
    "    font-weight: bold;\n",
    "    text-align: center;\">\n",
    "  Word Tokenization\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They', 'told', 'that', 'their', 'ages', 'of', 'mr.', 'are', '25,', '26.2', 'and', '31', 'respectively.']\n",
      "['They', 'told', 'that', 'their', 'ages', 'of', 'mr.', 'are', '25', ',', '26.2', 'and', '31', 'respectively', '.']\n"
     ]
    }
   ],
   "source": [
    "sent = 'They told that their ages of mr. are 25, 26.2 and 31 respectively.'\n",
    "print(sent.split())\n",
    "print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'friends!', 'How', 'are', 'you?', 'Welcome', 'to', 'the', 'world', 'of', 'python', 'programming.']\n",
      "['Hello', 'friends', '!', 'How', 'are', 'you', '?', 'Welcome', 'to', 'the', 'world', 'of', 'python', 'programming', '.']\n",
      "Percentage of punctuation 20.0%\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "sent = '''Hello friends!\n",
    "How are you? Welcome to the world of python programming.'''\n",
    "\n",
    "print(sent.split())\n",
    "print(word_tokenize(sent))\n",
    "\n",
    "ar = [1 if i in punctuation else 0 for i in word_tokenize(sent)]\n",
    "ar = sum(ar)/len(ar)\n",
    "\n",
    "print(f\"Percentage of punctuation {ar*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  background: linear-gradient(90deg,rgb(251, 255, 10), #ff758c, #ff4d6d);\n",
    "  -webkit-background-clip: text;\n",
    "  -webkit-text-fill-color: transparent;\n",
    "  font-size: 20px;\n",
    "  font-weight: bold;\n",
    "  text-align: center;\"\n",
    ">\n",
    "  Other Tokenizers\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['рдЫрддреНрд░рдкрддрд┐',\n",
       " 'рд╢рд┐рд╡рд╛рдЬреА',\n",
       " 'рдорд╣рд╛рд░рд╛рдЬ',\n",
       " 'рд╡рд╕реНрддреБ',\n",
       " 'рд╕рдВрдЧреНрд░рд╣рд╛рд▓рдп',\n",
       " 'рдореБрдВрдмрдИ',\n",
       " ',',\n",
       " '(',\n",
       " 'рдкреВрд░реНрд╡',\n",
       " 'рдирд╛рдо',\n",
       " \"'The\",\n",
       " 'Prince',\n",
       " 'of',\n",
       " 'Wales',\n",
       " 'Museum',\n",
       " 'of',\n",
       " 'Western',\n",
       " 'India',\n",
       " \"'\",\n",
       " ')',\n",
       " 'рдореБрдореНрдмрдИ',\n",
       " 'рдХрд╛',\n",
       " 'рдореБрдЦреНрдп',\n",
       " 'рд╕рдВрдЧреНрд░рд╣рд╛рд▓рдп',\n",
       " 'рд╣реИред',\n",
       " 'рдЗрд╕рдХрд╛',\n",
       " 'рдирд┐рд░реНрдорд╛рдг',\n",
       " 'рд╡реЗрд▓реНрд╕',\n",
       " 'рдХреЗ',\n",
       " 'рд░рд╛рдЬрдХреБрдорд╛рд░',\n",
       " 'рдХреЗ',\n",
       " 'рднрд╛рд░рдд',\n",
       " 'рдпрд╛рддреНрд░рд╛',\n",
       " 'рдХреЗ',\n",
       " 'рд╕рдордп',\n",
       " 'рдореБрдореНрдмрдИ',\n",
       " 'рдХреЗ',\n",
       " 'рдкреНрд░рддрд┐рд╖реНрдард┐рдд',\n",
       " 'рдЙрджреНрдпреЛрдЧрдкрддрд┐рдпреЛ',\n",
       " 'рдФрд░',\n",
       " 'рдирд╛рдЧрд░рд┐рдХреЛрдВ',\n",
       " 'рд╕реЗ',\n",
       " 'рдкреНрд░рд╛рдкреНрдд',\n",
       " 'рд╕рд╣рд╛рдпрддрд╛',\n",
       " 'рдореБрдореНрдмрдИ',\n",
       " 'рдХреЗ',\n",
       " 'рд╕рд░рдХрд╛рд░',\n",
       " 'рджреНрд╡рд╛рд░рд╛',\n",
       " 'рд╕реНрдорд╛рд░рдХ',\n",
       " 'рдХреЗ',\n",
       " 'рд░реВрдк',\n",
       " 'рдореЗрдВ',\n",
       " 'рдирд┐рд░реНрдорд┐рдд',\n",
       " 'рдХрд┐рдпрд╛',\n",
       " 'рдЧрдпрд╛',\n",
       " 'рдерд╛ред',\n",
       " 'рдпрд╣',\n",
       " 'рднрд╡реНрдп',\n",
       " 'рднрд╡рди',\n",
       " 'рджрдХреНрд╖рд┐рдг',\n",
       " 'рдореБрдореНрдмрдИ',\n",
       " 'рдХреЗ',\n",
       " 'рдлреЛрд░реНрдЯ',\n",
       " 'рд╡рд┐рд▓рд┐рдпрдо',\n",
       " 'рдореЗ',\n",
       " ',',\n",
       " 'рдПрд▓реНрдлрд┐рдВрд╕реНрдЯрди',\n",
       " 'рдХрд╛рд▓реЗрдЬ',\n",
       " 'рдХреЗ',\n",
       " 'рд╕рд╛рдордиреЗ',\n",
       " 'рд╣реИред',\n",
       " 'рдЗрд╕рдХреЗ',\n",
       " 'рд╕рд╛рдордиреЗ',\n",
       " 'рд░реАрдЧрд▓',\n",
       " 'рд╕рд┐рдирд┐рдорд╛',\n",
       " 'рдФрд░',\n",
       " 'рдкреБрд▓рд┐рд╕',\n",
       " 'рдЖрдпреБрдХреНрдд',\n",
       " 'рдХрд╛рд░реНрдпрд╛рд▓рдп',\n",
       " 'рд╕реНрдерд┐рдд',\n",
       " 'рд╣реИред',\n",
       " 'рдмрдЧрд▓',\n",
       " 'рдореЗ',\n",
       " \"'\",\n",
       " 'рдбреЕрд╡рд┐рдб',\n",
       " 'рд╕рд╕реВрди',\n",
       " 'рдкреБрд╕реНрддрдХ',\n",
       " 'рд╕рдВрдЧреНрд░рд╣рд╛рд▓рдп',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " 'рд╡реНрдпрд╛рдЯреНрд╕реНрди',\n",
       " 'рд╣реЙрдЯреЗрд▓реН',\n",
       " \"'\",\n",
       " ',',\n",
       " 'рднреА',\n",
       " 'рд╣реИред',\n",
       " 'рдЖрдЧреЗ',\n",
       " 'рд╕рдореБрджреНрд░',\n",
       " 'рдХреЗ',\n",
       " 'рддрд░рдл',\n",
       " 'рдЬрд╛рдиреЗ',\n",
       " 'рдкрд░',\n",
       " \"'\",\n",
       " 'рдЧреЗрдЯрд╡реЗ',\n",
       " 'рдСрдлрд╝',\n",
       " 'рдЗрдиреНрдбрд┐рдпрд╛',\n",
       " \"'\",\n",
       " 'рдЖрддрд╛',\n",
       " 'рд╣реИред']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = '''\n",
    "рдЫрддреНрд░рдкрддрд┐ рд╢рд┐рд╡рд╛рдЬреА рдорд╣рд╛рд░рд╛рдЬ рд╡рд╕реНрддреБ рд╕рдВрдЧреНрд░рд╣рд╛рд▓рдп рдореБрдВрдмрдИ, (рдкреВрд░реНрд╡ рдирд╛рдо 'The Prince of Wales Museum of Western India') рдореБрдореНрдмрдИ рдХрд╛ рдореБрдЦреНрдп рд╕рдВрдЧреНрд░рд╣рд╛рд▓рдп рд╣реИред \n",
    "рдЗрд╕рдХрд╛ рдирд┐рд░реНрдорд╛рдг рд╡реЗрд▓реНрд╕ рдХреЗ рд░рд╛рдЬрдХреБрдорд╛рд░ рдХреЗ рднрд╛рд░рдд рдпрд╛рддреНрд░рд╛ рдХреЗ рд╕рдордп рдореБрдореНрдмрдИ рдХреЗ рдкреНрд░рддрд┐рд╖реНрдард┐рдд рдЙрджреНрдпреЛрдЧрдкрддрд┐рдпреЛ рдФрд░ рдирд╛рдЧрд░рд┐рдХреЛрдВ рд╕реЗ рдкреНрд░рд╛рдкреНрдд рд╕рд╣рд╛рдпрддрд╛ рдореБрдореНрдмрдИ рдХреЗ рд╕рд░рдХрд╛рд░ рджреНрд╡рд╛рд░рд╛ рд╕реНрдорд╛рд░рдХ рдХреЗ рд░реВрдк рдореЗрдВ рдирд┐рд░реНрдорд┐рдд рдХрд┐рдпрд╛ рдЧрдпрд╛ рдерд╛ред \n",
    "рдпрд╣ рднрд╡реНрдп рднрд╡рди рджрдХреНрд╖рд┐рдг рдореБрдореНрдмрдИ рдХреЗ рдлреЛрд░реНрдЯ рд╡рд┐рд▓рд┐рдпрдо рдореЗ, рдПрд▓реНрдлрд┐рдВрд╕реНрдЯрди рдХрд╛рд▓реЗрдЬ рдХреЗ рд╕рд╛рдордиреЗ рд╣реИред \n",
    "рдЗрд╕рдХреЗ рд╕рд╛рдордиреЗ рд░реАрдЧрд▓ рд╕рд┐рдирд┐рдорд╛ рдФрд░ рдкреБрд▓рд┐рд╕ рдЖрдпреБрдХреНрдд рдХрд╛рд░реНрдпрд╛рд▓рдп рд╕реНрдерд┐рдд рд╣реИред\n",
    "рдмрдЧрд▓ рдореЗ 'рдбреЕрд╡рд┐рдб рд╕рд╕реВрди рдкреБрд╕реНрддрдХ рд╕рдВрдЧреНрд░рд╣рд╛рд▓рдп', 'рд╡реНрдпрд╛рдЯреНрд╕реНрди рд╣реЙрдЯреЗрд▓реН', рднреА рд╣реИред \n",
    "рдЖрдЧреЗ рд╕рдореБрджреНрд░ рдХреЗ рддрд░рдл рдЬрд╛рдиреЗ рдкрд░ 'рдЧреЗрдЯрд╡реЗ рдСрдлрд╝ рдЗрдиреНрдбрд┐рдпрд╛' рдЖрддрд╛ рд╣реИред\n",
    "'''\n",
    "\n",
    "word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['уГауГ│уГРуВдуБоуГБуГгуГИуГйуГСуГЖуВг',\n",
       " 'уВ╖уГ┤уВбуГ╝уВ╕уГ╝',\n",
       " 'уГЮуГПуГйуВ╕хНЪчЙйщди',\n",
       " '(',\n",
       " 'ф╗ехЙНуБпуАМше┐уВдуГ│уГЙуБоуГЧуГкуГ│уВ╣',\n",
       " 'уВкуГЦ',\n",
       " 'уВжуВзуГ╝уГлуВ║хНЪчЙйщдиуАНуБиуБЧуБжчЯеуВЙуВМуБжуБДуБ╛уБЧуБЯ',\n",
       " ')',\n",
       " 'уБпуАБуГауГ│уГРуВдуБоф╕╗шжБуБкхНЪчЙйщдиуБзуБЩуАВ',\n",
       " 'уБУуБошиШх┐╡чвСуБпуАБуВжуВзуГ╝уГлуВ║чЪЗхдкхнРуБоуВдуГ│уГЙшикхХПф╕нуБлуГауГ│уГРуВдцФ┐х║ЬуБМшСЧхРНуБкхоЯценхо╢уВДуГауГ│уГРуВдх╕Вц░СуБохНФхКЫуВТх╛ЧуБжшиШх┐╡чвСуБиуБЧуБжх╗║уБжуБЯуВВуБоуБзуБЩуАВ',\n",
       " 'уБУуБохгохдзуБкх╗║чЙйуБпуАБхНЧуГауГ│уГРуВдуБоуГХуВйуГ╝уГИ',\n",
       " 'уВжуВгуГкуВвуГауАБуВиуГлуГХуВгуГ│уВ╣уГИуГ╝уГ│хдзхнжуБохРСуБЛуБДуБлуБВуВКуБ╛уБЩуАВ',\n",
       " 'уГкуГ╝уВмуГлуВ╖уГНуГЮуБишнжхпЯх║БшИОуБМчЫоуБохЙНуБлуБВуВКуБ╛уБЩуАВ',\n",
       " 'ш┐СуБПуБлуБпуАМуГЗуГ┤уВгуГГуГЙуГ╗уВ╡уВ╣уГ╝уГ│уГ╗уГЦуГГуВпуГ╗уГЯуГеуГ╝уВ╕уВвуГауАНуАБуАМуГ┤уГгуГИуГ│уГ╗уГЫуГЖуГлуАНуВВуБВуВКуБ╛уБЩуАВ',\n",
       " 'уБХуВЙуБлц╡╖уБоуБ╗уБЖуБ╕шбМуБПуБиуАМуВдуГ│уГЙщЦАуАНуБМуБВуВКуБ╛уБЩуАВ']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"\"\"\n",
    "уГауГ│уГРуВдуБоуГБуГгуГИуГйуГСуГЖуВг уВ╖уГ┤уВбуГ╝уВ╕уГ╝ уГЮуГПуГйуВ╕хНЪчЙйщди (ф╗ехЙНуБпуАМше┐уВдуГ│уГЙуБоуГЧуГкуГ│уВ╣ уВкуГЦ уВжуВзуГ╝уГлуВ║хНЪчЙйщдиуАНуБиуБЧуБжчЯеуВЙуВМуБжуБДуБ╛уБЧуБЯ) уБпуАБуГауГ│уГРуВдуБоф╕╗шжБуБкхНЪчЙйщдиуБзуБЩуАВ\n",
    "уБУуБошиШх┐╡чвСуБпуАБуВжуВзуГ╝уГлуВ║чЪЗхдкхнРуБоуВдуГ│уГЙшикхХПф╕нуБлуГауГ│уГРуВдцФ┐х║ЬуБМшСЧхРНуБкхоЯценхо╢уВДуГауГ│уГРуВдх╕Вц░СуБохНФхКЫуВТх╛ЧуБжшиШх┐╡чвСуБиуБЧуБжх╗║уБжуБЯуВВуБоуБзуБЩуАВ\n",
    "уБУуБохгохдзуБкх╗║чЙйуБпуАБхНЧуГауГ│уГРуВдуБоуГХуВйуГ╝уГИ уВжуВгуГкуВвуГауАБуВиуГлуГХуВгуГ│уВ╣уГИуГ╝уГ│хдзхнжуБохРСуБЛуБДуБлуБВуВКуБ╛уБЩуАВ\n",
    "уГкуГ╝уВмуГлуВ╖уГНуГЮуБишнжхпЯх║БшИОуБМчЫоуБохЙНуБлуБВуВКуБ╛уБЩуАВ\n",
    "ш┐СуБПуБлуБпуАМуГЗуГ┤уВгуГГуГЙуГ╗уВ╡уВ╣уГ╝уГ│уГ╗уГЦуГГуВпуГ╗уГЯуГеуГ╝уВ╕уВвуГауАНуАБуАМуГ┤уГгуГИуГ│уГ╗уГЫуГЖуГлуАНуВВуБВуВКуБ╛уБЩуАВ\n",
    "уБХуВЙуБлц╡╖уБоуБ╗уБЖуБ╕шбМуБПуБиуАМуВдуГ│уГЙщЦАуАНуБМуБВуВКуБ╛уБЩуАВ\"\"\"\n",
    "\n",
    "word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  background: linear-gradient(90deg,rgb(251, 255, 10), #ff758c, #ff4d6d);\n",
    "  -webkit-background-clip: text;\n",
    "  -webkit-text-fill-color: transparent;\n",
    "  font-size: 20px;\n",
    "  font-weight: bold;\n",
    "  text-align: center;\"\n",
    ">\n",
    "  Senetence Tokenizers\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends!',\n",
       " 'How are you?',\n",
       " 'Welcome to the world of python programming.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = '''Hello friends!\n",
    "How are you? Welcome to the world of python programming.'''\n",
    "\n",
    "sent_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\nрдЫрддреНрд░рдкрддрд┐ рд╢рд┐рд╡рд╛рдЬреА рдорд╣рд╛рд░рд╛рдЬ рд╡рд╕реНрддреБ рд╕рдВрдЧреНрд░рд╣рд╛рд▓рдп рдореБрдВрдмрдИ, (рдкреВрд░реНрд╡ рдирд╛рдо 'The Prince of Wales Museum of Western India') рдореБрдореНрдмрдИ рдХрд╛ рдореБрдЦреНрдп рд╕рдВрдЧреНрд░рд╣рд╛рд▓рдп рд╣реИ.\",\n",
       " 'рдЗрд╕рдХрд╛ рдирд┐рд░реНрдорд╛рдг рд╡реЗрд▓реНрд╕ рдХреЗ рд░рд╛рдЬрдХреБрдорд╛рд░ рдХреЗ рднрд╛рд░рдд рдпрд╛рддреНрд░рд╛ рдХреЗ рд╕рдордп рдореБрдореНрдмрдИ рдХреЗ рдкреНрд░рддрд┐рд╖реНрдард┐рдд рдЙрджреНрдпреЛрдЧрдкрддрд┐рдпреЛ рдФрд░ рдирд╛рдЧрд░рд┐рдХреЛрдВ рд╕реЗ рдкреНрд░рд╛рдкреНрдд рд╕рд╣рд╛рдпрддрд╛ рдореБрдореНрдмрдИ рдХреЗ рд╕рд░рдХрд╛рд░ рджреНрд╡рд╛рд░рд╛ рд╕реНрдорд╛рд░рдХ рдХреЗ рд░реВрдк рдореЗрдВ рдирд┐рд░реНрдорд┐рдд рдХрд┐рдпрд╛ рдЧрдпрд╛ рдерд╛.',\n",
       " 'рдпрд╣ рднрд╡реНрдп рднрд╡рди рджрдХреНрд╖рд┐рдг рдореБрдореНрдмрдИ рдХреЗ рдлреЛрд░реНрдЯ рд╡рд┐рд▓рд┐рдпрдо рдореЗ, рдПрд▓реНрдлрд┐рдВрд╕реНрдЯрди рдХрд╛рд▓реЗрдЬ рдХреЗ рд╕рд╛рдордиреЗ рд╣реИ.',\n",
       " 'рдЗрд╕рдХреЗ рд╕рд╛рдордиреЗ рд░реАрдЧрд▓ рд╕рд┐рдирд┐рдорд╛ рдФрд░ рдкреБрд▓рд┐рд╕ рдЖрдпреБрдХреНрдд рдХрд╛рд░реНрдпрд╛рд▓рдп рд╕реНрдерд┐рдд рд╣реИ.',\n",
       " \"рдмрдЧрд▓ рдореЗ 'рдбреЕрд╡рд┐рдб рд╕рд╕реВрди рдкреБрд╕реНрддрдХ рд╕рдВрдЧреНрд░рд╣рд╛рд▓рдп', 'рд╡реНрдпрд╛рдЯреНрд╕реНрди рд╣реЙрдЯреЗрд▓реН', рднреА рд╣реИ.\",\n",
       " \"рдЖрдЧреЗ рд╕рдореБрджреНрд░ рдХреЗ рддрд░рдл рдЬрд╛рдиреЗ рдкрд░ 'рдЧреЗрдЯрд╡реЗ рдСрдлрд╝ рдЗрдиреНрдбрд┐рдпрд╛' рдЖрддрд╛ рд╣реИ.\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = '''\n",
    "рдЫрддреНрд░рдкрддрд┐ рд╢рд┐рд╡рд╛рдЬреА рдорд╣рд╛рд░рд╛рдЬ рд╡рд╕реНрддреБ рд╕рдВрдЧреНрд░рд╣рд╛рд▓рдп рдореБрдВрдмрдИ, (рдкреВрд░реНрд╡ рдирд╛рдо 'The Prince of Wales Museum of Western India') рдореБрдореНрдмрдИ рдХрд╛ рдореБрдЦреНрдп рд╕рдВрдЧреНрд░рд╣рд╛рд▓рдп рд╣реИ.\n",
    "рдЗрд╕рдХрд╛ рдирд┐рд░реНрдорд╛рдг рд╡реЗрд▓реНрд╕ рдХреЗ рд░рд╛рдЬрдХреБрдорд╛рд░ рдХреЗ рднрд╛рд░рдд рдпрд╛рддреНрд░рд╛ рдХреЗ рд╕рдордп рдореБрдореНрдмрдИ рдХреЗ рдкреНрд░рддрд┐рд╖реНрдард┐рдд рдЙрджреНрдпреЛрдЧрдкрддрд┐рдпреЛ рдФрд░ рдирд╛рдЧрд░рд┐рдХреЛрдВ рд╕реЗ рдкреНрд░рд╛рдкреНрдд рд╕рд╣рд╛рдпрддрд╛ рдореБрдореНрдмрдИ рдХреЗ рд╕рд░рдХрд╛рд░ рджреНрд╡рд╛рд░рд╛ рд╕реНрдорд╛рд░рдХ рдХреЗ рд░реВрдк рдореЗрдВ рдирд┐рд░реНрдорд┐рдд рдХрд┐рдпрд╛ рдЧрдпрд╛ рдерд╛.\n",
    "рдпрд╣ рднрд╡реНрдп рднрд╡рди рджрдХреНрд╖рд┐рдг рдореБрдореНрдмрдИ рдХреЗ рдлреЛрд░реНрдЯ рд╡рд┐рд▓рд┐рдпрдо рдореЗ, рдПрд▓реНрдлрд┐рдВрд╕реНрдЯрди рдХрд╛рд▓реЗрдЬ рдХреЗ рд╕рд╛рдордиреЗ рд╣реИ.\n",
    "рдЗрд╕рдХреЗ рд╕рд╛рдордиреЗ рд░реАрдЧрд▓ рд╕рд┐рдирд┐рдорд╛ рдФрд░ рдкреБрд▓рд┐рд╕ рдЖрдпреБрдХреНрдд рдХрд╛рд░реНрдпрд╛рд▓рдп рд╕реНрдерд┐рдд рд╣реИ.\n",
    "рдмрдЧрд▓ рдореЗ 'рдбреЕрд╡рд┐рдб рд╕рд╕реВрди рдкреБрд╕реНрддрдХ рд╕рдВрдЧреНрд░рд╣рд╛рд▓рдп', 'рд╡реНрдпрд╛рдЯреНрд╕реНрди рд╣реЙрдЯреЗрд▓реН', рднреА рд╣реИ. \n",
    "рдЖрдЧреЗ рд╕рдореБрджреНрд░ рдХреЗ рддрд░рдл рдЬрд╛рдиреЗ рдкрд░ 'рдЧреЗрдЯрд╡реЗ рдСрдлрд╝ рдЗрдиреНрдбрд┐рдпрд╛' рдЖрддрд╛ рд╣реИ.\n",
    "'''\n",
    "\n",
    "sent_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the max sentence which has the most the word 'the'\n",
    "sent = \"\"\"India, officially the Republic of India,[j][20] is a country in South Asia. It is the seventh-largest country in the world by area and the most populous country. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west;[k] China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand, Myanmar, and Indonesia.\n",
    "\n",
    "Modern humans arrived on the Indian subcontinent from Africa no later than 55,000 years ago.[22][23][24] Their long occupation, initially in varying forms of isolation as hunter-gatherers, has made the region highly diverse, second only to Africa in human genetic diversity.[25] Settled life emerged on the subcontinent in the western margins of the Indus river basin 9,000 years ago, evolving gradually into the Indus Valley Civilisation of the third millennium BCE.[26] By at least 1200 BCE, an archaic form of Sanskrit, an Indo-European language, had diffused into India from the northwest.[27][28] Its evidence today is found in the hymns of the Rigveda. Preserved by an oral tradition that was resolutely vigilant, the Rigveda records the dawning of Hinduism in India.[29] The Dravidian languages of India were supplanted in the northern and western regions.[30] By 400 BCE, stratification and exclusion by caste had emerged within Hinduism,[31] and Buddhism and Jainism had arisen, proclaiming social orders unlinked to heredity.[32] Early political consolidations gave rise to the loose-knit Maurya and Gupta Empires based in the Ganges Basin.[33] Their collective era was suffused with wide-ranging creativity,[34] but also marked by the declining status of women,[35] and the incorporation of untouchability into an organised system of belief.[l][36] The Middle kingdoms exported Sanskrit language, south Indian scripts and religions of Hinduism and Buddhism to the Southeast Asia.\n",
    "\"\"\"\n",
    "sents = sent_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west;[k] China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = {i : j.lower().count('the')  for i,j in enumerate(sents)}\n",
    "\n",
    "idex = sorted(res.items(), key=lambda x: x[1], reverse=True)\n",
    "display(idex[0][1], sents[idex[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west;[k] China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(\n",
    "    lambda x, y : x if (x.lower().count('the') > y.lower().count('the')) else y,\n",
    "    sents\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  background: linear-gradient(90deg,rgb(251, 255, 10), #ff758c, #ff4d6d);\n",
    "  -webkit-background-clip: text;\n",
    "  -webkit-text-fill-color: transparent;\n",
    "  font-size: 20px;\n",
    "  font-weight: bold;\n",
    "  text-align: center;\"\n",
    ">\n",
    "  Whitespace Tokenizers\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'python',\n",
       " 'programming.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent = '''Hello friends!\n",
    "How are you? Welcome to the world of python programming.'''\n",
    "tk = WhitespaceTokenizer()\n",
    "\n",
    "\n",
    "display(tk.tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  background: linear-gradient(90deg,rgb(251, 255, 10), #ff758c, #ff4d6d);\n",
    "  -webkit-background-clip: text;\n",
    "  -webkit-text-fill-color: transparent;\n",
    "  font-size: 20px;\n",
    "  font-weight: bold;\n",
    "  text-align: center;\"\n",
    ">\n",
    "Space Tokenizers\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import SpaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends!\\nHow',\n",
       " 'are',\n",
       " 'you?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'python',\n",
       " 'programming.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends!\\nHow',\n",
       " 'are',\n",
       " 'you?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'python',\n",
       " 'programming.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent = '''Hello friends!\n",
    "How are you? Welcome to the world of python programming.'''\n",
    "\n",
    "\n",
    "# Only take spaces not the newline\n",
    "tk = SpaceTokenizer()\n",
    "\n",
    "display(sent.split(' '))\n",
    "display(tk.tokenize(sent)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  background: linear-gradient(90deg,rgb(251, 255, 10), #ff758c, #ff4d6d);\n",
    "  -webkit-background-clip: text;\n",
    "  -webkit-text-fill-color: transparent;\n",
    "  font-size: 20px;\n",
    "  font-weight: bold;\n",
    "  text-align: center;\"\n",
    ">\n",
    "Line Tokenizers\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import LineTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends!', 'How are you? Welcome to the world of python programming.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Hello friends!', 'How are you? Welcome to the world of python programming.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent = '''Hello friends!\n",
    "How are you? Welcome to the world of python programming.'''\n",
    "\n",
    "\n",
    "# Only take newline\n",
    "tk = LineTokenizer()\n",
    "\n",
    "display(sent.split('\\n'))\n",
    "display(tk.tokenize(sent)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  background: linear-gradient(90deg,rgb(251, 255, 10), #ff758c, #ff4d6d);\n",
    "  -webkit-background-clip: text;\n",
    "  -webkit-text-fill-color: transparent;\n",
    "  font-size: 20px;\n",
    "  font-weight: bold;\n",
    "  text-align: center;\"\n",
    ">\n",
    "Tab Tokenizers\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TabTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends!\\nHow are you? Welcome to',\n",
       " 'the world of python',\n",
       " 'programming.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Hello friends!\\nHow are you? Welcome to',\n",
       " 'the world of python',\n",
       " 'programming.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent = '''Hello friends!\n",
    "How are you? Welcome to\\tthe world of python\\tprogramming.'''\n",
    "\n",
    "\n",
    "# Only take tab\n",
    "tk = TabTokenizer()\n",
    "\n",
    "display(sent.split('\\t'))\n",
    "display(tk.tokenize(sent)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  background: linear-gradient(90deg,rgb(251, 255, 10), #ff758c, #ff4d6d);\n",
    "  -webkit-background-clip: text;\n",
    "  -webkit-text-fill-color: transparent;\n",
    "  font-size: 20px;\n",
    "  font-weight: bold;\n",
    "  text-align: center;\"\n",
    ">\n",
    "Tweet Tokenizers\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'ЁЯлг',\n",
       " 'friends',\n",
       " '!',\n",
       " ':',\n",
       " ')',\n",
       " 'ЁЯТА',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " ':',\n",
       " '$',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'pythonЁЯРН',\n",
       " 'programming.',\n",
       " '<',\n",
       " '3']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'ЁЯлг',\n",
       " 'friends',\n",
       " '!',\n",
       " ':)',\n",
       " 'ЁЯТА',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " ':',\n",
       " '$',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'python',\n",
       " 'ЁЯРН',\n",
       " 'programming',\n",
       " '.',\n",
       " '<3']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent = '''Hello ЁЯлг friends!:)ЁЯТА\n",
    "How are you? Welcome :$ to\\tthe world of pythonЁЯРН\\tprogramming.<3'''\n",
    "\n",
    "tk = TweetTokenizer()\n",
    "# It tokenizes based on the emojis and emoji generator keycombinations( :) smile, :( sad face, <3 heart, etc )\n",
    " \n",
    "display(word_tokenize(sent))\n",
    "display(tk.tokenize(sent)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  background: linear-gradient(90deg,rgb(251, 255, 10), #ff758c, #ff4d6d);\n",
    "  -webkit-background-clip: text;\n",
    "  -webkit-text-fill-color: transparent;\n",
    "  font-size: 20px;\n",
    "  font-weight: bold;\n",
    "  text-align: center;\"\n",
    ">\n",
    "Multi Word Extension Tokenizers\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Van Rossom',\n",
       " 'is',\n",
       " 'in',\n",
       " 'Pune',\n",
       " '.',\n",
       " 'We',\n",
       " 'welcomed',\n",
       " 'Van Rossom',\n",
       " 'here',\n",
       " '.',\n",
       " 'Van',\n",
       " 'Nayak',\n",
       " 'is',\n",
       " 'also',\n",
       " 'in',\n",
       " 'pune',\n",
       " 'doing',\n",
       " 'Majdoori',\n",
       " 'in',\n",
       " 'CDAC']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'Van Rossom is in Pune. We welcomed Van Rossom here. Van Nayak is also in pune doing Majdoori in CDAC'\n",
    "\n",
    "# A tokenizer that processes tokenized text and merges multi-word expressions into single tokens.\n",
    "tk = MWETokenizer(separator=' ') \n",
    "\n",
    "tk.add_mwe(('Van', 'Rossom'))\n",
    "tk.tokenize(word_tokenize(sent)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  background: linear-gradient(90deg,rgb(251, 255, 10), #ff758c, #ff4d6d);\n",
    "  -webkit-background-clip: text;\n",
    "  -webkit-text-fill-color: transparent;\n",
    "  font-size: 20px;\n",
    "  font-weight: bold;\n",
    "  text-align: center;\"\n",
    ">\n",
    "Custom Tokenizers\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Van',\n",
       " 'Rossom',\n",
       " 'is',\n",
       " 'in',\n",
       " 'Pune',\n",
       " 'We',\n",
       " 'welcomed',\n",
       " 'Van',\n",
       " 'Rossom',\n",
       " 'here',\n",
       " 'Van',\n",
       " 'Nayak',\n",
       " 'in',\n",
       " '>',\n",
       " 'pune',\n",
       " '&',\n",
       " 'doing',\n",
       " 'Majdoori',\n",
       " 'in',\n",
       " 'CDAC']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent = 'Van Rossom is in Pune. We welcomed Van Rossom here!. Van Nayak in > pune & doing Majdoori in CDAC'\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return re.split(r\"[.,;?!\\s]+\", text)\n",
    "\n",
    "tokens = custom_tokenizer(sent)\n",
    "display(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
